

Les systèmes d'IA, comme tout autre logiciel, sont sujets à des vulnérabilités qui peuvent être exploitées par les attaquants. Voici quelques types et préoccupations clés :

Apprentissage par machine de l'adversaire : Les attaquants peuvent délibérément manipuler des données ou des modèles d'entraînement à l'IA pour produire des résultats biaisés, inexacts ou malveillants.

Empoisonnement Attaques : des données malveillantes injectées dans des ensembles de données d'entraînement à l'IA peuvent compromettre l'intégrité du modèle et conduire à des prédictions ou décisions incorrectes.

Évasion Attaques: Les attaquants peuvent fabriquer des entrées spécifiquement conçues pour échapper aux systèmes de sécurité basés sur l'IA, comme la détection d'intrusion ou la détection de logiciels malveillants.

Explicabilité et transparence : Le manque de transparence dans les processus décisionnels d'IA peut rendre difficile l'identification et la résolution des vulnérabilités.

Inversion du modèle Attaques : Les attaquants peuvent inverser les modèles AI pour extraire des informations sensibles, telles que des données d'entraînement ou des paramètres de modèle.

Empoisonnement des données : Les données malveillantes injectées dans les systèmes d'IA peuvent compromettre leur fonctionnalité, leur précision ou leur sécurité.

Absence de développement sûr Pratiques: Il se peut que les systèmes d'IA ne soient pas développés dans l'esprit de la sécurité, ce qui les rende vulnérables à l'exploitation.

## Stratégies d'atténuation

Collecte et étiquetage sécurisés des données : Assurez-vous que les données sont recueillies et étiquetées de façon sécuritaire afin de prévenir les attaques d'empoisonnement.

Vérification régulière des modèles : Évaluer périodiquement les modèles d'IA pour les biais, les inexactitudes et les vulnérabilités.

Validation des entrées : Mettre en œuvre une validation des entrées robuste pour détecter et prévenir les attaques d'évasion.

Explicable AI: Développer des systèmes d'IA en ayant à l'esprit la transparence et l'explicabilité pour faciliter l'identification et l'atténuation de la vulnérabilité.

Déploiement sécurisé et isolement : Déployer des systèmes d'IA dans des environnements sécurisés et les isoler de données et de réseaux sensibles.

Surveillance continue : Surveiller régulièrement les systèmes d'IA pour détecter les signes de compromis ou d'anomalies.

Collaboration et information Partage : Encourager la collaboration et l'échange d'information entre les concepteurs, les chercheurs et les experts en sécurité de l'IA afin de rester en avance sur les nouvelles menaces.

## Exemples du monde réel

**SAPwned**: Wiz Research a découvert des vulnérabilités dans SAP AI Core, permettant aux attaquants de reprendre le service et d'accéder aux données des clients.

**Vulnérabilités de sécurité de Cisco AI** : Cisco définit les vulnérabilités de sécurité en matière d'IA et d'apprentissage automatique comme des faiblesses exploitables dans les modèles d'IA, les logiciels ou le code matériel qui affectent négativement la confidentialité, l'intégrité ou la disponibilité.

**NIST identifie Cyberattaques** : L'Institut national des normes et de la technologie (NIST) a identifié des types de cyberattaques qui manipulent le comportement des systèmes d'IA, y compris les menaces d'apprentissage automatique contradictoires.